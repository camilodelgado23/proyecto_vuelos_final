{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este Jupyter notebook insertamos el dataset original y el transformado en las tablas que creamos en el Jupyter notebook tablas_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cambiamos el directorio raíz del proyecto para facilitar la carga de los datasets y las importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio de trabajo actual después del cambio: /Users/admin/Documents/gustavo/Proyecto_Vuelos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "root_dir = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "os.chdir(root_dir)\n",
    "\n",
    "print(\"Directorio de trabajo actual después del cambio:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importamos las Librerias y la conexion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conexión exitosa a la base de datos PostgreSQL.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "from Database.conexion_BD import create_connection\n",
    "\n",
    "connection = create_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crear el engine de SQLAlchemy usando la conexión existente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "postgresql_engine = create_engine('postgresql+psycopg2://', creator=lambda: connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos el Dataset Original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ruta_dataset_original \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcsv/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(ruta_dataset_original, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m train\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "ruta_dataset_original = 'csv/train.csv'\n",
    "train = pd.read_csv(ruta_dataset_original, delimiter=';', on_bad_lines='skip')\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remplazamos las comas por puntos para que los datos con notacion cientifica no interrumpan este proceso, y ajustamos los nombres de las columnas para que coincidan con los nombres de la tabla en PostgreSQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train is not None:\n",
    "    for col in train.select_dtypes(include=['object']).columns:\n",
    "        train[col] = train[col].str.replace(',', '.', regex=False)\n",
    "\n",
    "    train.columns = [\n",
    "        'month', 'day_of_week', 'dep_del15', 'dep_time_blk', 'distance_group', 'segment_number',\n",
    "        'concurrent_flights', 'number_of_seats', 'carrier_name', 'airport_flights_month',\n",
    "        'airline_flights_month', 'airline_airport_flights_month', 'avg_monthly_pass_airport',\n",
    "        'avg_monthly_pass_airline', 'flt_attendants_per_pass', 'ground_serv_per_pass', 'plane_age',\n",
    "        'departing_airport', 'latitude', 'longitude', 'previous_airport', 'prcp', 'snow', 'snwd',\n",
    "        'tmax', 'awnd', 'carrier_historical', 'dep_airport_hist', 'day_historical', 'dep_block_hist'\n",
    "    ]\n",
    "else:\n",
    "    print(\"No se pudo leer el archivo CSV, no se insertarán datos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lo insertamos en la tabla (Debido a la cantidad de datos es normal que se demore un poco al ejecutar el codigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos insertados en la base de datos exitosamente.\n"
     ]
    }
   ],
   "source": [
    "if train is not None and not train.empty:\n",
    "    try:\n",
    "        train.to_sql('flights', con=postgresql_engine, if_exists='append', index=False)\n",
    "        print(\"Datos insertados en la base de datos exitosamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al insertar datos: {e}\")\n",
    "else:\n",
    "    print(\"El DataFrame está vacío, no se insertarán datos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cragamos el Dataset Transformado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_dataset_original = 'csv/flights_limpio.csv'\n",
    "train_limpio = pd.read_csv(ruta_dataset_original, delimiter=',', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lo insertamos en la tabla (Debido a la cantidad de datos es normal que se demore un poco al ejecutar el codigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos insertados en la base de datos exitosamente.\n"
     ]
    }
   ],
   "source": [
    "if train_limpio is not None and not train_limpio.empty:\n",
    "    try:\n",
    "        train_limpio.to_sql('flights_limpio', con=postgresql_engine, if_exists='append', index=False)\n",
    "        print(\"Datos insertados en la base de datos exitosamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al insertar datos: {e}\")\n",
    "else:\n",
    "    print(\"El DataFrame está vacío, no se insertarán datos.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
